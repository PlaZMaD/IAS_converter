{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kic2W_9uzCGp"
      ],
      "authorship_tag": "ABX9TyOF8m8eb6/x9C+lMZBbUI6f"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#hidden block"
      ],
      "metadata": {
        "id": "kic2W_9uzCGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install plotly-resampler dateparser >> /dev/null\n",
        "# %pip install --index-url https://public:{key}@gitlab.com/api/v4/projects/55331319/packages/pypi/simple --no-deps bglabutils==0.0.21 >> /dev/null\n",
        "%pip install --index-url https://gitlab.com/api/v4/projects/55331319/packages/pypi/simple --no-deps bglabutils==0.0.21 >> /dev/null"
      ],
      "metadata": {
        "id": "whzZHpWKjUPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg3CMzDdVQA-"
      },
      "outputs": [],
      "source": [
        "!rm log.log\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from google.colab import files\n",
        "import io, pkgutil\n",
        "\n",
        "import bglabutils.basic as bg\n",
        "import bglabutils.filters as bf\n",
        "\n",
        "import ipywidgets\n",
        "from IPython.display import display, Javascript, clear_output\n",
        "from ipywidgets import Checkbox, widgets\n",
        "import time\n",
        "import os\n",
        "\n",
        "import logging\n",
        "import re\n",
        "import sys\n",
        "\n",
        "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, filename=\"/content/log.log\", filemode=\"w\", force=True)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
        "logging.info(\"START\")\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yzsGkFOEQwkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {}\n",
        "config['debug'] = False  # True загрузит небольшой кусок файла, а не целый\n",
        "config['-9999_to_nan'] = True #заменяем -9999  на np.nan\n",
        "config['repair_time'] = True #генерируем новые временные метки в случае ошибок\n",
        "\n",
        "#####################\n",
        "#на случай сложных колонок времени\n",
        "config['time'] = {}\n",
        "config['time']['column_name'] = 'datetime'\n",
        "def my_datetime_converter(x):\n",
        "    date = x['date'].astype(str) #x['date'].dt.strftime('%d.%m.%Y') if is_datetime(x['date'].dtype) else x['date'].astype(str)\n",
        "    time = x['time'].astype(str) #x['time'].dt.strftime('%H:%M') if is_datetime(x['time'].dtype) else x['time'].astype(str)\n",
        "\n",
        "    x['tmp_datetime'] = date + \" \" + time\n",
        "    #Проверить формат даты-времени в FullOutput\n",
        "    format = \"%d.%m.%Y %H:%M\"#\"%d/%m/%Y %H:%M\"# \"%Y-%m-%d %H:%M\"  #\"%d/%m/%Y %H:%M\"  #\"%Y-%m-%d %H:%M:%S\"\n",
        "    return pd.to_datetime(x['tmp_datetime'], format=format)#dayfirst=True)#, format=format)\n",
        "config['time']['converter'] = my_datetime_converter\n",
        "#####################\n",
        "\n",
        "###Запишите название Ваших файлов и путь к ним. Если файлы будут импортированы с google-диска\n",
        "###через команду !gdown, то достаточно заменить название файла\n",
        "config['path'] = ['eddy_pro result_SSB 2023.csv']#['eddypro_GHG_biomet_CO2SS_Express_full_output_2023-03-29T020107_exp.csv']#['eddypro_noHMP_full_output_2014_1-5.csv', 'eddypro_noHMP_full_output_2014_5-12.csv']#['/content/eddypro_NCT_GHG_22-23dry_full_output.xlsx', '/content/eddypro_NCT_GHG_22wet_full_output.xlsx', '/content/eddypro_NCT_GHG_23wet_full output.xlsx']#'/content/new.csv'\n",
        "# config['path'] = '/content/DT_Full output.xlsx'"
      ],
      "metadata": {
        "id": "iNiWHl0XmmeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #@title Select file to process\n",
        "# filename = \"selct file\" #@param {type:\"file\"}\n",
        "# password = \"root\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "mqAZKssaVYq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_df(d_config, sheet_name=None):\n",
        "    logging.info(f\"{d_config['path']} is processing\")\n",
        "\n",
        "    if isinstance(d_config['path'], list) or isinstance(d_config['path'], tuple):\n",
        "        multi_out = []\n",
        "        time = None\n",
        "        for file in d_config['path']:\n",
        "            temp_config = d_config.copy()\n",
        "            temp_config['path'] = file\n",
        "            loaded_data, time = load_df(temp_config)\n",
        "            multi_out = multi_out + [df for df in loaded_data.values()]\n",
        "        freqs = [df.index.freq for df in  loaded_data.values()]\n",
        "        if not np.all(np.array(freqs) == freqs[0]):\n",
        "            logging.info(\"Different freq in data files. Aborting!\")\n",
        "            return None\n",
        "\n",
        "        multi_out = pd.concat(multi_out)\n",
        "        multi_out = multi_out.sort_index()\n",
        "        multi_out = repair_time(multi_out, time)\n",
        "        return {'default':multi_out}, time\n",
        "\n",
        "    file_name = d_config['path']#os.path.normpath(d_config['path'])\n",
        "    _, file_extension = os.path.splitext(file_name)\n",
        "    ext = 'csv' if '.csv' in file_extension.lower() else None\n",
        "    ext = 'excel' if file_extension.lower() in ['.xls', '.xlsx'] else ext\n",
        "\n",
        "    load_func = None\n",
        "    l_config = {}\n",
        "\n",
        "    if 'debug' in d_config.keys():\n",
        "        if d_config['debug'] == True and 'nrows' not in l_config.keys():\n",
        "            l_config['nrows'] = 500\n",
        "\n",
        "    if ext == 'csv':\n",
        "        load_func = pd.read_csv\n",
        "    if ext == 'excel':\n",
        "        load_func = pd.read_excel\n",
        "        l_config['sheet_name'] = sheet_name\n",
        "\n",
        "    if not load_func:\n",
        "        logging.info(f\"Wrong file extension, got {ext}, need csv, xls or xlsx\")\n",
        "        return None\n",
        "    tmp_config = l_config.copy()\n",
        "    tmp_config['nrows'] = 4\n",
        "    tmp_config['header'] = None\n",
        "    # tmp_config['engine'] = 'openpyxl'\n",
        "    df_dict = None\n",
        "    with open(file_name, encoding='utf8', errors='backslashreplace') as f:\n",
        "        df_dict = load_func(f, **tmp_config)\n",
        "\n",
        "    if ext == 'csv':\n",
        "        df_dict = {'default': df_dict}\n",
        "\n",
        "    l_config['skiprows'] = []\n",
        "    skipped = 1\n",
        "    for key, item in df_dict.items():\n",
        "        item = item.dropna(how='all', axis=1)\n",
        "        if (item.loc[0, :].isnull()).sum() > 2:\n",
        "            logging.info(f\"skipping lines 1, 3\")\n",
        "            l_config['skiprows'] = [0, 2]  # .append(i)\n",
        "            skipped = skipped + 2\n",
        "            continue\n",
        "\n",
        "        item = item.replace('NAN', np.nan)\n",
        "        item = item.dropna(how='all', axis=1)\n",
        "        cond_1 = np.isreal(item.loc[1, :].to_numpy()) == False#(item.loc[1, :].astype(str).str.isnumeric() == False)#(item.loc[0, item.columns != d_config['time']].astype(str).str.isnumeric() == False)\n",
        "        cond_2 = (item.loc[1, :].isna())#(item.loc[0, item.columns != d_config['time']].isna())\n",
        "\n",
        "\n",
        "        if (cond_1.sum() > 1 and cond_2.sum() !=len(item.columns) - 1) or (cond_1.sum()==0 and cond_2.sum()==0): #cond_1.sum() > 1 and cond_2.sum() != len(item.columns) - 1: #any(item.loc[0, item.columns != d_config['time']].astype(str).str.isnumeric() == False) and not all(item.loc[0, item.columns != d_config['time']].isna()):\n",
        "            l_config['skiprows'].append(1)#[1]#lambda x: x == 1\n",
        "            skipped = skipped + 1\n",
        "            logging.info(f\"skipping line {2}\")\n",
        "            # else:\n",
        "            #     pass\n",
        "            #     # l_config['skiprows'] = None\n",
        "\n",
        "    with open(file_name, encoding='utf8', errors='backslashreplace') as f:\n",
        "        df_dict = load_func(f, **l_config)\n",
        "    if ext == 'csv':\n",
        "        df_dict = {'default': df_dict}\n",
        "\n",
        "    if 'time' not in d_config:\n",
        "        test_df = next(iter(df_dict.values()))\n",
        "        tmp_time = [col for col in test_df.columns if is_datetime(test_df[col].dtype)]\n",
        "        if len(tmp_time) == 1:\n",
        "            logging.info(f'Time column detected, {tmp_time[0]}')\n",
        "            # d_config['time'] = tmp_time[0]\n",
        "        else:\n",
        "            logging.info(\"can't detect datetime column, specify one in the config!\")\n",
        "            return None\n",
        "\n",
        "    else:\n",
        "        tmp_time = [d_config['time']['column_name']]\n",
        "        for key, item in df_dict.items():\n",
        "            item[tmp_time[0]] = d_config['time']['converter'](item) #pd.to_datetime(item[tmp_time[0]], format=d_config['time']['format'])\n",
        "\n",
        "    if d_config['-9999_to_nan']:\n",
        "        logging.info('Replacing -9999 to np.nan')\n",
        "        for key, item in df_dict.items():\n",
        "            item.replace(to_replace=-9999, value=np.nan, inplace=True)\n",
        "\n",
        "\n",
        "    for key, item in df_dict.items():\n",
        "\n",
        "        # if not item.index.is_monotonic_increasing:\n",
        "        #     logging.info(f'WARNING the time-index is not monotonic for {key}!')\n",
        "        # Проверяем время на монотонность\n",
        "        item['default_index'] = item.index + skipped + 1\n",
        "        item.dropna(how='all', axis=0, inplace=True)\n",
        "        correct_number_of_time_entries = pd.date_range(item[tmp_time[0]].iloc[0], item[tmp_time[0]].iloc[-1],\n",
        "                                                       freq=item[tmp_time[0]].iloc[1] - item[tmp_time[0]].iloc[0])\n",
        "\n",
        "        # if not correct_number_of_time_entries.size == len(item.index):\n",
        "        #     logging.info(\"Missing time values\")\n",
        "\n",
        "        # if item[tmp_time[0]].is_monotonic_increasing:\n",
        "        #     logging.info(f\"The time in {key} looks fine\")\n",
        "        # else:\n",
        "        #     logging.info(f\"The time is not monotonic in {key}\")\n",
        "        #     test_data = item.copy()\n",
        "        #     test_data['shift'] = test_data[tmp_time[0]].shift(1)\n",
        "        #     test_data['diff'] = test_data[tmp_time[0]] - test_data['shift']\n",
        "        #     logging.info(\"Try to check near: \", test_data.loc[~(test_data['diff'] > np.timedelta64(20, 's')), tmp_time[0]])\n",
        "        if d_config['repair_time']:\n",
        "            logging.info(\"Fixing time\")\n",
        "            # item[tmp_time[0]] = pd.date_range(item[tmp_time[0]].iloc[0], item[tmp_time[0]].iloc[-1],\n",
        "            #                            freq=item[tmp_time[0]].iloc[1] - item[tmp_time[0]].iloc[0])\n",
        "            df_dict[key] = repair_time(item, tmp_time[0])\n",
        "        item.index = item[tmp_time[0]]\n",
        "\n",
        "    return df_dict, tmp_time[0]\n",
        "\n",
        "def get_freq(df, time):\n",
        "    try_max = 100\n",
        "    try_ind = 0\n",
        "    t_shift = 5\n",
        "    start = 1\n",
        "    deltas = df[time] - df[time].shift(1)\n",
        "    while try_ind < try_max:\n",
        "        del_arr = deltas.iloc[start + try_ind*t_shift : start + try_ind*t_shift + t_shift].values\n",
        "        if not np.all(del_arr == del_arr[0]) and del_arr[0] is not None:\n",
        "            try_ind = try_ind + 1\n",
        "            continue\n",
        "        else:\n",
        "            return del_arr[0]\n",
        "\n",
        "def repair_time(df, time):\n",
        "    freq = get_freq(df, time)\n",
        "    df = df.set_index(time, drop=False)\n",
        "    tmp_index = df.index.copy()\n",
        "    df = df[~df.index.duplicated(keep='first')]\n",
        "\n",
        "    if not tmp_index.equals(df.index):\n",
        "        print(\"Duplicated indexes! check lines:\", tmp_index[tmp_index.duplicated() == True])\n",
        "\n",
        "    start = 0\n",
        "    stop = -1\n",
        "    while True:\n",
        "        try:\n",
        "            pd.to_datetime(df[time].iloc[start])\n",
        "            break\n",
        "        except:\n",
        "            start = start + 1\n",
        "            continue\n",
        "    while True:\n",
        "        try:\n",
        "            pd.to_datetime(df[time].iloc[stop])\n",
        "            break\n",
        "        except:\n",
        "            stop = stop - 1\n",
        "            continue\n",
        "    new_time = pd.DataFrame(index=pd.date_range(start=df[time].iloc[start], end=df[time].iloc[stop],\n",
        "                                                freq=pd.to_timedelta(freq)))\n",
        "    # new_time[time] = new_time.index\n",
        "    new_time = new_time.join(df, how='left')\n",
        "    # new_time[time] = new_time[time+'_new']\n",
        "    # new_time = new_time.drop(time+'_new', axis=1)\n",
        "    return new_time"
      ],
      "metadata": {
        "id": "Hk-xVpXabTwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_time(data_in, time_in, metadata, check_year=True):\n",
        "  outflag = 0\n",
        "  data_freq = pd.to_timedelta(get_freq(data_in, time_in))\n",
        "\n",
        "  logging.info(f\"Checking tme column for {metadata['name']}\")\n",
        "  correct_column_type = is_datetime(data_in[time_in])\n",
        "  if not correct_column_type:\n",
        "    logging.info(f\"Cold not find the Time column in {metadata['name']}\")\n",
        "    outflag = 1\n",
        "\n",
        "  missed_values = data_in[time_in].isna()\n",
        "  if missed_values.any():\n",
        "    logging.error(f\"Can't read timestamp. Please check entries in {metadata['name']} near lines \\n {data_in.loc[missed_values, 'default_index'].to_numpy()}\")\n",
        "    outflag = 1\n",
        "\n",
        "  if check_year and (not (data_in[time_in].dt.year.to_numpy()[0] == data_in[time_in].dt.year.to_numpy()).all()) and (not outflag):\n",
        "    logging.error(f\"There should be only one year presented in {metadata['name']}!\")\n",
        "    outflag = 1\n",
        "\n",
        "  correct_number_of_time_entries = pd.date_range(data_in[time_in].iloc[0], data_in[time_in].iloc[-1],\n",
        "                                                       freq=data_freq)\n",
        "\n",
        "  if not correct_number_of_time_entries.size == len(data_in.index):\n",
        "    print(correct_number_of_time_entries.size , len(data_in.index))\n",
        "    logging.error(\"Number of entries is not correct\")\n",
        "    outflag = 1\n",
        "\n",
        "\n",
        "  if data_in.index.duplicated(keep='first').any():\n",
        "      logging.error(f\"Duplicated indexes! check lines:{ data_in.loc[data_in.index.duplicated(), 'default_index'].to_numpy()}\")\n",
        "      outflag = 1\n",
        "\n",
        "  start = 0\n",
        "  stop = -1\n",
        "  while True:\n",
        "        try:\n",
        "            pd.to_datetime(data_in[time_in].iloc[start])\n",
        "            break\n",
        "        except:\n",
        "            start = start + 1\n",
        "            continue\n",
        "  while True:\n",
        "      try:\n",
        "          pd.to_datetime(data_in[time_in].iloc[stop])\n",
        "          break\n",
        "      except:\n",
        "          stop = stop - 1\n",
        "          continue\n",
        "\n",
        "  correct_index = pd.date_range(start=data_in[time_in].iloc[start], end=data_in[time_in].iloc[stop],\n",
        "                                                freq=pd.to_timedelta(data_freq))\n",
        "  extra_index = data_in.index.difference(correct_index)\n",
        "  missing_index = correct_index.difference(data_in.index)\n",
        "  if len(missing_index)>0:\n",
        "    outflag = 1\n",
        "    logging.error(f\"Missing values: {missing_index.astype(str)}\")\n",
        "  if len(extra_index)>0:\n",
        "    outflag = 1\n",
        "    logging.error(f\"Extra values: {extra_index.astype(str)}, lines: {data_in.loc[extra_index, 'default_index'].to_numpy()}\")\n",
        "\n",
        "  logging.info(\"\\n\")\n",
        "\n",
        "  return outflag\n",
        "\n",
        "def check_compatibility(data_1, data_2, time, config_1, config_2):\n",
        "  out_flag = check_time(biomet_data, time, biomet_config)\n",
        "  out_flag = out_flag + check_time(data, time, config)\n",
        "\n",
        "  if out_flag !=0:\n",
        "    return out_flag\n",
        "\n",
        "  if pd.to_timedelta(get_freq(data_1, time)) != pd.to_timedelta(get_freq(data_2, time)):\n",
        "    out_flag = 1\n",
        "    logging.error(f\"Different time freqs in files!\")\n",
        "    return out_flag\n",
        "\n",
        "  index_dif_1 = data_1.index.difference(data_2.index)\n",
        "  index_dif_2 = data_2.index.difference(data_1.index)\n",
        "  if len(index_dif_1) > 0:\n",
        "    out_flag = 1\n",
        "    logging.error(f\"Different timestamps in files! Check in{config_1['name']} near line {data_1.loc[index_dif_1, ['default_index', time]].to_numpy()[0]}\")\n",
        "  if len(index_dif_2) > 0:\n",
        "    out_flag = 1\n",
        "    logging.error(f\"Different timestamps in files! Check in{config_2['name']} near line{data_2.loc[index_dif_2, ['default_index', time]].to_numpy()[0]}\")\n",
        "  if out_flag !=0:\n",
        "    return out_flag\n",
        "\n",
        "  return out_flag"
      ],
      "metadata": {
        "id": "rcw9pEyHtn8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing(data_in):\n",
        "  data_in.columns= data_in.columns.str.lower()\n",
        "\n",
        "  have_rh_flag = False\n",
        "  have_vpd_flag = False\n",
        "  have_par_flag = False\n",
        "  have_swin_flag = False\n",
        "  have_rg_flag = False\n",
        "  have_p_flag = False\n",
        "  have_pr_flag = False\n",
        "  have_ppfd_flag = False\n",
        "\n",
        "  for col_name in data_in.columns:\n",
        "    if 'u*' in col_name:\n",
        "      logging.info(f\"renaming {col_name} to u_star\")\n",
        "      data_in = data_in.rename(columns={col_name: 'u_star'})\n",
        "    if 'ppfd_in_1_1_1' in col_name:\n",
        "      logging.info(f\"renaming {col_name} to ppfd_1_1_1\")\n",
        "      data_in = data_in.rename(columns={col_name: 'ppfd_1_1_1'})\n",
        "    if 'sw_in_1_1_1' in col_name:\n",
        "      logging.info(f\"renaming {col_name} to swin_1_1_1\")\n",
        "      data_in = data_in.rename(columns={col_name: 'swin_1_1_1'})\n",
        "    if 'co2_signal_strength' in col_name:\n",
        "      logging.info(f\"renaming {col_name} to co2_signal_strength\")\n",
        "      data_in = data_in.rename(columns={col_name: 'co2_signal_strength'})\n",
        "    if \"rh_1_1_1\" in col_name:\n",
        "      have_rh_flag =True\n",
        "    if \"vpd_1_1_1\" in col_name:\n",
        "      have_vpd_flag = True\n",
        "    if 'swin' in col_name or 'sw_in' in col_name:\n",
        "      have_swin_flag = True\n",
        "    if 'par' in col_name:\n",
        "      have_par_flag = True\n",
        "    if 'rg_1_1_1' in col_name:\n",
        "      have_rg_flag = True\n",
        "    if 'p_1_1_1' in col_name:\n",
        "      have_p_flag = True\n",
        "    if 'p_rain_1_1_1' in col_name:\n",
        "      have_pr_flag = True\n",
        "    if 'ppfd_1_1_1' in col_name:\n",
        "      have_ppfd_flag = True\n",
        "    if col_name in ['co2_signal_strength_7500_mean', 'CO2SS'.lower()] or 'co2_signal_strength' in col_name:\n",
        "      logging.info(f\"renaming {col_name} to co2_signal_strength\")\n",
        "      data_in = data_in.rename(columns={col_name: 'co2_signal_strength'})\n",
        "    if col_name in ['ch4_signal_strength_7700_mean', 'CH4SS'.lower()] or 'ch4_signal_strength' in col_name:\n",
        "      logging.info(f\"renaming {col_name} to ch4_signal_strength\")\n",
        "      data_in = data_in.rename(columns={col_name: 'ch4_signal_strength'})\n",
        "\n",
        "\n",
        "  if not (have_ppfd_flag or have_swin_flag):\n",
        "    logging.info(\"NO PPFD and SWin\")\n",
        "  else:\n",
        "      if not have_ppfd_flag:\n",
        "        data_in['ppfd_1_1_1'] = data_in['swin_1_1_1'] / 0.47\n",
        "      if not have_swin_flag:\n",
        "        data_in['swin_1_1_1'] = 0.47 * data_in['ppfd_1_1_1']\n",
        "      have_ppfd_flag = True\n",
        "      have_swin_flag = True\n",
        "\n",
        "\n",
        "  if not (have_rg_flag or have_swin_flag):\n",
        "    logging.info(\"NO RG AND SWIN\")\n",
        "  else:\n",
        "    logging.info(\"Checking RG-SWIN pair\")\n",
        "    if not have_rg_flag:\n",
        "      data_in['rg_1_1_1'] = data_in['swin_1_1_1']\n",
        "    if not have_swin_flag:\n",
        "      data_in['swin_1_1_1'] = data_in['rg_1_1_1']\n",
        "      have_swin_flag = True\n",
        "\n",
        "\n",
        "  if not (have_p_flag or have_pr_flag):\n",
        "    logging.info(\"NO P and P_RAIN\")\n",
        "  else:\n",
        "    logging.info(\"Checking P <-> P_rain pair\")\n",
        "    if not have_p_flag:\n",
        "      data_in['p_1_1_1'] = data_in['p_rain_1_1_1']\n",
        "    if not have_pr_flag:\n",
        "      data_in['p_rain_1_1_1'] = data_in['p_1_1_1']\n",
        "\n",
        "\n",
        "  if not (have_vpd_flag or have_rh_flag):\n",
        "    logging.info(\"NO RH AND VPD\")\n",
        "  else:\n",
        "      temp_k = (data_in['ta_1_1_1'] + 273.15)\n",
        "      logE = 23.5518-(2937.4/temp_k)-4.9283*np.log10(temp_k)\n",
        "      ehpa = np.power(10, logE)\n",
        "      if not have_vpd_flag:\n",
        "        logging.info(\"calculating vpd_1_1_1 from rh\")\n",
        "        data_in['vpd_1_1_1'] = ehpa - (ehpa*data_in['rh_1_1_1']/100)\n",
        "      if not have_rh_flag:\n",
        "        logging.info(\"calculating rh_1_1_1 from vpd\")\n",
        "        data_in['rh_1_1_1'] = ehpa\n",
        "\n",
        "\n",
        "  if not (have_par_flag or have_swin_flag):\n",
        "    logging.info(\"NO PAR and SWin\")\n",
        "  else:\n",
        "      if not have_par_flag:\n",
        "        data_in['par'] = data_in['swin_1_1_1'] / 0.47#SWin=PAR*0.47\n",
        "      if not have_swin_flag:\n",
        "        data_in['swin_1_1_1'] = 0.47 * data_in['par']\n",
        "\n",
        "\n",
        "\n",
        "  for col_name in ['co2_signal_strength_7500_mean', 'CO2SS'.lower()]:\n",
        "    # logging.info(data_in.columns.to_list())\n",
        "    if col_name in data_in.columns.to_list():\n",
        "      logging.info(f\"renaming {col_name} to co2_signal_strength\")\n",
        "      data_in = data_in.rename(columns={col_name: 'co2_signal_strength'})\n",
        "\n",
        "  for col_name in ['ch4_signal_strength_7700_mean', 'CH4SS'.lower()]:\n",
        "    # logging.info(data_in.columns.to_list())\n",
        "    if col_name in data_in.columns.to_list():\n",
        "      logging.info(f\"renaming {col_name} to ch4_signal_strength\")\n",
        "      data_in = data_in.rename(columns={col_name: 'ch4_signal_strength'})\n",
        "  return data_in"
      ],
      "metadata": {
        "id": "x4oBU-LsvQmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "box_val = False\n",
        "loop_flag = True\n",
        "\n",
        "box = Checkbox(box_val, description='datetime in a single column')\n",
        "\n",
        "datetime_col_name = widgets.Text(\n",
        "    value='TIMESTAMP_START',\n",
        "    placeholder='datetime',\n",
        "    description='DT column:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "time_template = widgets.Text(\n",
        "    value='%d.%m.%Y %H:%M',\n",
        "    placeholder='DT format',\n",
        "    description='DT format:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "\n",
        "date_col_name = widgets.Text(\n",
        "    value='date',\n",
        "    placeholder='Date',\n",
        "    description='Date column:',\n",
        "    disabled=False\n",
        ")\n",
        "time_col_name = widgets.Text(\n",
        "    value='time',\n",
        "    placeholder='time',\n",
        "    description='Time column:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "button = widgets.Button(description=\"Ready\", disabled=False)\n",
        "\n",
        "def changed_box(b):\n",
        "\n",
        "  global box_val\n",
        "\n",
        "  if box_val != b['owner'].value:\n",
        "    # display(Javascript('notebook.select_next(); notebook.execute_cell();echo(111)'))\n",
        "    box_val = b['owner'].value\n",
        "    if box.value == True:\n",
        "      clear_output(wait=True)\n",
        "      display(box, datetime_col_name, time_template, button)\n",
        "    else:\n",
        "      clear_output(wait=True)\n",
        "      display(box, date_col_name, time_col_name, time_template, button)\n",
        "      # display\n",
        "\n",
        "def run_forward(b):\n",
        "  global loop_flag\n",
        "  loop_flag = False\n",
        "  # display(Javascript('document.dispatchEvent(new KeyboardEvent(\"keydown\", {key: \"F10\", code: \"F10\", ctrlKey: true, keyCode: 121}));'))\n",
        "\n",
        "box.observe(changed_box)\n",
        "button.on_click(run_forward)\n"
      ],
      "metadata": {
        "id": "t8D86-s5zkwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "biomet_box_val = False\n",
        "\n",
        "biomet_box = Checkbox(biomet_box_val, description='datetime in a single column')\n",
        "\n",
        "biomet_datetime_col_name = widgets.Text(\n",
        "    value='TIMESTAMP_1',\n",
        "    placeholder='datetime',\n",
        "    description='DT column:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "biomet_time_template = widgets.Text(\n",
        "    value='%Y-%m-%d %H%M',\n",
        "    placeholder='DT format',\n",
        "    description='DT format:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "\n",
        "biomet_date_col_name = widgets.Text(\n",
        "    value='Date',\n",
        "    placeholder='Date',\n",
        "    description='Date column:',\n",
        "    disabled=False\n",
        ")\n",
        "biomet_time_col_name = widgets.Text(\n",
        "    value='Time',\n",
        "    placeholder='time',\n",
        "    description='Time column:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "\n",
        "def biomet_changed_box(b):\n",
        "\n",
        "  global biomet_box_val\n",
        "\n",
        "  if biomet_box_val != b['owner'].value:\n",
        "    # display(Javascript('notebook.select_next(); notebook.execute_cell();echo(111)'))\n",
        "    biomet_box_val = b['owner'].value\n",
        "    if biomet_box.value == True:\n",
        "      clear_output(wait=True)\n",
        "      display(biomet_box, biomet_datetime_col_name, biomet_time_template)\n",
        "    else:\n",
        "      clear_output(wait=True)\n",
        "      display(box, date_col_name, time_col_name, time_template)\n",
        "      # display\n",
        "\n",
        "biomet_box.observe(biomet_changed_box)"
      ],
      "metadata": {
        "id": "BCBHemc9uGGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "station_name_field = widgets.Text(\n",
        "    value='FY4',\n",
        "    placeholder='Station name',\n",
        "    description='Station name',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "station_version_field = widgets.Text(\n",
        "    value='v01',\n",
        "    placeholder='File version',\n",
        "    description='File version',\n",
        "    disabled=False\n",
        ")"
      ],
      "metadata": {
        "id": "MDLrLk_ccSUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#actual start"
      ],
      "metadata": {
        "id": "pvrP-dnIzGrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload your data\n",
        "data_file = files.upload()\n",
        "biomet_file = files.upload()\n",
        "# display(uploader)\n",
        "\n",
        "# display(box, date_col_name, time_col_name, time_template)\n",
        "# display(biomet_box, biomet_date_col_name, biomet_time_col_name, biomet_time_template)\n",
        "# display(station_name_field, station_version_field)"
      ],
      "metadata": {
        "id": "h9raYLwFXjAz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Full_Output title time config, space will be added in case of date+time columns\n",
        "display(box, date_col_name, time_col_name, time_template)\n",
        "#raise ValueError('breakpoint')"
      ],
      "metadata": {
        "id": "2k35jXPzxtVe",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Biomet title time config, space will be added in case of date+time columns\n",
        "display(biomet_box, biomet_date_col_name, biomet_time_col_name, biomet_time_template)"
      ],
      "metadata": {
        "id": "DKBrwBzvvae5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Output name config\n",
        "display(station_name_field, station_version_field)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dXCU2yFqcJeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка и обработка данных"
      ],
      "metadata": {
        "id": "mHbj7edzsSmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Загрузка FullOutput\n",
        "config = {}\n",
        "config['debug'] = False  # True загрузит небольшой кусок файла, а не целый\n",
        "config['-9999_to_nan'] = True #заменяем -9999  на np.nan\n",
        "config['repair_time'] = False #генерируем новые временные метки в случае ошибок\n",
        "config['name'] = 'FullOutput'\n",
        "\n",
        "\n",
        "if box.value:\n",
        "  config['time'] = {}\n",
        "  config['time']['column_name'] = 'my_datetime'\n",
        "  def my_datetime_converter(x):\n",
        "    format = time_template.value\n",
        "    return pd.to_datetime(x[datetime_col_name.value], format=format)\n",
        "  config['time']['converter'] = my_datetime_converter\n",
        "\n",
        "else:\n",
        "  config['time'] = {}\n",
        "  config['time']['column_name'] = 'my_datetime'\n",
        "  def my_datetime_converter(x):\n",
        "      date = x[date_col_name.value].astype(str)\n",
        "      time = x[time_col_name.value].astype(str)\n",
        "      x['tmp_datetime'] = date + \" \" + time\n",
        "      format = time_template.value\n",
        "      return pd.to_datetime(x['tmp_datetime'], format=format, errors='coerce')\n",
        "\n",
        "  config['time']['converter'] = my_datetime_converter\n",
        "\n",
        "config['path'] = next(iter( data_file.keys()))\n",
        "\n",
        "data, time = load_df(config)\n",
        "data = data[next(iter(data))]  #т.к. изначально у нас словарь\n",
        "data_freq = data.index.freq\n",
        "logging.info(f\"{config['name']} loaded. Datetime interval: {data.index.min()} - {data.index.max()}\")"
      ],
      "metadata": {
        "id": "aYPEYmSlpsSw",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Загрузка Biomet\n",
        "biomet_config = {}\n",
        "biomet_config['name'] = 'Biomet'\n",
        "biomet_config['debug'] = False  # True загрузит небольшой кусок файла, а не целый\n",
        "biomet_config['-9999_to_nan'] = True #заменяем -9999  на np.nan\n",
        "biomet_config['repair_time'] = False #генерируем новые временные метки в случае ошибок\n",
        "\n",
        "\n",
        "if biomet_box.value:\n",
        "  biomet_config['time'] = {}\n",
        "  biomet_config['time']['column_name'] = 'my_datetime'\n",
        "  def my_datetime_converter(x):\n",
        "    format = biomet_time_template.value\n",
        "    return pd.to_datetime(x[biomet_datetime_col_name.value], format=format, errors='coerce')\n",
        "  biomet_config['time']['converter'] = my_datetime_converter\n",
        "\n",
        "else:\n",
        "  biomet_config['time'] = {}\n",
        "  biomet_config['time']['column_name'] = 'my_datetime'\n",
        "  def my_datetime_converter(x):\n",
        "      date = x[biomet_date_col_name.value].astype(str)\n",
        "      time = x[biomet_time_col_name.value].astype(str)\n",
        "      x['tmp_datetime'] = date + \" \" + time\n",
        "      format = biomet_time_template.value\n",
        "      return pd.to_datetime(x['tmp_datetime'], format=format)\n",
        "\n",
        "  biomet_config['time']['converter'] = my_datetime_converter\n",
        "\n",
        "biomet_config['path'] = next(iter( biomet_file.keys()))\n",
        "\n",
        "biomet_data, time  = load_df(biomet_config)\n",
        "biomet_data = biomet_data[next(iter(biomet_data))]  #т.к. изначально у нас словарь\n",
        "logging.info(f\"{biomet_config['name']} loaded. Datetime interval: {biomet_data.index.min()} - {biomet_data.index.max()}\")"
      ],
      "metadata": {
        "id": "QuTSGCNQhLvJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Проверка данных\n",
        "out_flag = check_compatibility(data, biomet_data, time, config, biomet_config)\n",
        "assert out_flag==0, \"ERROR, check log file\""
      ],
      "metadata": {
        "id": "34znBRq0bs7x",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title предобработка данных\n",
        "combined_data = data.join(biomet_data, how='outer', rsuffix='_meteo')\n",
        "try:\n",
        "  combined_data.index.freq = pd.to_timedelta(get_freq(combined_data, time))\n",
        "except:\n",
        "  logging.error(\"Failed to set the correct frequency\")\n",
        "  assert False, \"Error, check the log file\"\n",
        "\n",
        "combined_data =  data_preprocessing(combined_data)"
      ],
      "metadata": {
        "id": "gxhugJzMMSmd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Сохранение данных\n",
        "ias_df = data.copy()\n",
        "ias_df = ias_df.fillna(-9999)\n",
        "\n",
        "col_match =  {\"co2_flux\" : \"FC_1_1_1\", \"qc_co2_flux\" : \"FC_SSITC_TEST_1_1_1\", \"LE\" : \"LE_1_1_1\",\n",
        "  \"qc_LE\" : \"LE_SSITC_TEST_1_1_1\", \"H\" : \"H_1_1_1\", \"qc_H\" : \"H_SSITC_TEST_1_1_1\", \"Tau\" : \"TAU_1_1_1\",\n",
        "  \"qc_Tau\" : \"TAU_SSITC_TEST_1_1_1\", \"co2_strg\" : \"SC_1_1_1\", \"co2_mole_fraction\" : \"CO2_1_1_1\",\n",
        "  \"h2o_mole_fraction\" : \"H2O_1_1_1\", \"sonic_temperature\" : \"T_SONIC_1_1_1\", \"u*\" : \"USTAR_1_1_1\",\n",
        "  \"Ta_1_1_1\" : \"TA_1_1_1\", \"Pa_1_1_1\" : \"PA_1_1_1\", \"Swin_1_1_1\" : \"SW_IN_1_1_1\", \"Swout_1_1_1\" : \"SW_OUT_1_1_1\",\n",
        "  \"Lwin_1_1_1\" : \"LW_IN_1_1_1\", \"Lwout_1_1_1\" : \"LW_OUT_1_1_1\", \"PPFD_1_1_1\" : \"PPFD_IN_1_1_1\",\n",
        "  \"Rn_1_1_1\" : \"NETRAD_1_1_1\", \"MWS_1_1_1\" : \"WS_1_1_1\", \"Ts_1_1_1\" : \"TS_1_1_1\", \"Ts_2_1_1\" : \"TS_2_1_1\",\n",
        "  \"Ts_3_1_1\" : \"TS_3_1_1\", \"Pswc_1_1_1\" : \"SWC_1_1_1\", \"Pswc_2_1_1\" : \"SWC_2_1_1\", \"Pswc_3_1_1\" : \"SWC_3_1_1\",\n",
        "  \"SHF_1_1_1\" : \"G_1_1_1\", \"SHF_2_1_1\" : \"G_2_1_1\", \"SHF_3_1_1\" : \"G_3_1_1\", \"L\" : \"MO_LENGTH_1_1_1\",\n",
        "  \"(z-d)/L\" : \"ZL_1_1_1\", \"x_peak\" : \"FETCH_MAX_1_1_1\", \"x_70%\" : \"FETCH_70_1_1_1\", \"x_90%\" : \"FETCH_90_1_1_1\", \"ch4_flux\" : \"FCH4_1_1_1\", \"qc_ch4_flux\" : \"FCH4_SSITC_TEST_1_1_1\", \"ch4_mole_fraction\" : \"CH4_1_1_1\", \"ch4_strg\" : \"SCH4_1_1_1\",  \"ch4_signal_strength\" : \"CH4_RSSI_1_1_1\", \"co2_signal_strength\" : \"CO2_STR_1_1_1\"}\n",
        "col_match = {key.lower(): item for key, item in col_match.items()}\n",
        "\n",
        "ias_df = ias_df.rename(columns=col_match)\n",
        "time_cols = ['TIMESTAMP_START', 'TIMESTAMP_END', 'DTime']\n",
        "var_cols = [col_match[col] for col in col_match.keys() if col_match[col] in ias_df.columns]\n",
        "\n",
        "new_time = pd.DataFrame(index=pd.date_range(start=f\"01.01.{ias_df[time].dt.year.min()}\", end=f\"01.01.{ias_df[time].dt.year.max()}\",\n",
        "                                              freq=ias_df.index.freq, inclusive='left'))\n",
        "ias_df = new_time.join(ias_df, how='left')\n",
        "ias_df[time] = ias_df.index\n",
        "\n",
        "ias_df['TIMESTAMP_START'] = ias_df[time].dt.strftime('%Y%m%d%H%M')\n",
        "ias_df['TIMESTAMP_END'] = (ias_df[time] + pd.Timedelta(0.5, \"h\")).dt.strftime('%Y%m%d%h%M')\n",
        "ias_df['DTime'] = np.round(ias_df[time].dt.dayofyear + 1./48*2*ias_df[time].dt.hour + 1./48*(ias_df[time].dt.minute//30), decimals=3)\n",
        "\n",
        "if 'h_strg' in ias_df.columns:\n",
        "  ias_df['SH'] = ias_df['h_strg']\n",
        "  var_cols.append('SH')\n",
        "if 'le_strg' in ias_df.columns:\n",
        "  ias_df['SLE'] = ias_df['le_strg']\n",
        "  var_cols.append('SLE')\n",
        "\n",
        "if 'SW_IN_1_1_1' in ias_df.columns:\n",
        "  ias_df['SW_IN_1_1_1'] = data['swin_1_1_1']\n",
        "\n",
        "ias_year = ias_df[time].dt.year.min()\n",
        "var_cols.sort()\n",
        "col_list_ias = time_cols + var_cols + [time]\n",
        "print(col_list_ias)\n",
        "ias_df = ias_df[col_list_ias]\n",
        "\n",
        "for year in ias_df.index.year.unique():\n",
        "  ias_filename = f\"{station_name_field.value}_{ias_year}_{station_version_field.value}.csv\"\n",
        "  save_data = ias_df.loc[ias_df[time].dt.year==year]\n",
        "  save_data = save_data.drop(time, axis=1)\n",
        "  save_data = save_data.fillna(-9999)\n",
        "  if len(save_data.index) >= 5:\n",
        "    save_data.to_csv(os.path.join('output',ias_filename), index=False)\n",
        "    logging.info(f\"IAS file saved to {os.path.join(ias_filename)}.csv\")\n",
        "  else:\n",
        "    try:\n",
        "      os.remove(os.path.join(ias_filename))\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "\n",
        "    # print(f\"not enough data for {year}\")\n",
        "    logging.info(f\"{year} not saved, not enough data!\")\n"
      ],
      "metadata": {
        "id": "8LqXAYk6NgjI",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}